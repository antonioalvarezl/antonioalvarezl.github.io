<!DOCTYPE html>
<html lang="en">
  <!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2016 -->
  
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis</title>

  <meta name="author" content="Antonio Álvarez-López" />

  

  <link rel="alternate" type="application/rss+xml"
    title="Antonio Álvarez-López - Personal Website" href="/feed.xml" />

  

  
  
  
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.0/css/font-awesome.min.css" />

  
  

  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" />
  
  <link rel="stylesheet" href="/css/bootstrap-social.css" />
  
  <link rel="stylesheet" href="/css/main.css" />
  
  

  <link rel="preload" as="image" href="/assets/images/buceo.JPG" />

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:300,400,500,700,900" />
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto+Condensed:300,400,700" />
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" />
  
  

  

  

  

  <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis" />
  

  
  <meta property="og:description" content="This post describes how to reproduce and extend our pipeline of speech separation, diarization and ASR. We have provided separated audio files along with entire end-to-end reproducible recipes to supplement [our SLT 2021 paper](https://arxiv.org/abs/2011.02014). Here is a summary of contents of this post: 1. [Summary of our pipeline](#pipeline) 2. [Datasets](#data)...">
  


  <meta property="og:type" content="website" />

  
  <meta property="og:url" content="http://localhost:4000/pages/jsalt/" />
  <link rel="canonical" href="http://localhost:4000/pages/jsalt/" />
  

  
  <meta property="og:image" content="http://localhost:4000/static/img/antonio.jpg" />
  


  <!-- Twitter summary cards -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@" />
  <meta name="twitter:creator" content="@" />

  
  <meta name="twitter:title" content="Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis" />
  

  
  <meta name="twitter:description" content="This post describes how to reproduce and extend our pipeline of speech separation, diarization and ASR. We have provided separated audio files along with entire end-to-end reproducible recipes to supplement [our SLT 2021 paper](https://arxiv.org/abs/2011.02014). Here is a summary of contents of this post: 1. [Summary of our pipeline](#pipeline) 2. [Datasets](#data)...">
  

  
  <meta name="twitter:image" content="http://localhost:4000/static/img/antonio.jpg" />
  

  <script
    type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    type="text/javascript"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">


  <meta name="theme-color" content="#ffffff">

  <style>
    html {
      background-color: #cfd8e3;
    }
    body {
      background-image: url("/assets/images/buceo.JPG");
      background-size: cover;
      background-position: center;
      background-attachment: fixed;
      background-repeat: no-repeat;
      background-color: rgba(255, 255, 255, 0.8);
      background-blend-mode: overlay;
    }
  </style>

</head>


  <body>

    
  
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
        <a class="navbar-brand" href="http://localhost:4000">Antonio Álvarez-López</a>
      
      <div class="navbar-social" aria-label="Social links">
        
          <a href="https://github.com/antonioalvarezl" title="GitHub" aria-label="GitHub">
            <i class="fa fa-github" aria-hidden="true"></i>
          </a>
        
        
          <a href="https://linkedin.com/in/antonio-alvarez-lopez-6a09851b4" title="LinkedIn" aria-label="LinkedIn">
            <i class="fa fa-linkedin" aria-hidden="true"></i>
          </a>
        
        
          <a href="https://orcid.org/0009-0004-1302-6389" title="ORCID" aria-label="ORCID">
            <i class="ai ai-orcid" aria-hidden="true"></i>
          </a>
        
        
          <a href="https://scholar.google.com/citations?user=4Xm2yLgAAAAJ" title="Google Scholar" aria-label="Google Scholar">
            <i class="ai ai-google-scholar" aria-hidden="true"></i>
          </a>
        
        
          <a href="https://www.researchgate.net/profile/Antonio-Alvarez-Lopez-2" title="ResearchGate" aria-label="ResearchGate">
            <i class="ai ai-researchgate" aria-hidden="true"></i>
          </a>
        
      </div>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
          
            <li>
              
            





<a href="/">Home</a>

            </li>
          
        
      
        
          
            <li>
              
            





<a href="/papers">Papers</a>

            </li>
          
        
      
        
      
        
          
            <li>
              
            





<a href="/code">Code</a>

            </li>
          
        
      
        
          
            <li>
              
            





<a href="/talks">Talks</a>

            </li>
          
        
      
        
          
            <li>
              
            





<a href="/cv">CV</a>

            </li>
          
        
      
      
        
      
        
      
        
          
            <li class="nav-item-blog">
              
            





<a href="/blog">Blog</a>

            </li>
          
        
      
        
      
        
      
        
      
      </ul>
    </div>

	
	<div class="avatar-container">
	  <div class="avatar-img-border">
	    <a href="http://localhost:4000 ">
	      <img class="avatar-img" src="/static/img/antonio.jpg" />
		</a>
	  </div>
	</div>
	

  </div>
</nav>


    <main id="swup" class="page-transition">
      <!-- TODO this file has become a mess, refactor it -->





<header class="header-section ">

<div class="intro-header no-img">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-heading">
          <h1>Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis</h1>
		  
		  
		  
        </div>
      </div>
    </div>
  </div>
</div>
</header>




<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <p>This post describes how to reproduce and extend our pipeline of speech separation,
diarization and ASR. We have provided separated audio files along with entire
end-to-end reproducible recipes to supplement <a href="https://arxiv.org/abs/2011.02014">our SLT 2021 paper</a>.</p>

<p>Here is a summary of contents of this post:</p>

<ol>
  <li><a href="#pipeline">Summary of our pipeline</a></li>
  <li><a href="#data">Datasets</a></li>
  <li><a href="#recipe">Reproducible recipes</a></li>
  <li><a href="#example">Example use cases (for extending this work)</a></li>
  <li><a href="#credits">Credits</a></li>
</ol>

<p><a name="pipeline"></a></p>

<h3 id="summary-of-our-pipeline">Summary of our pipeline</h3>

<p>Our speech processing pipeline consists of the following 3 stages in sequence. For
each of these stages, we experimented with the methods mentioned below.</p>

<ol>
  <li>
    <p><strong>Speech separation</strong></p>

    <ol>
      <li>Mask-based MVDR</li>
      <li>Sequential neural beamforming</li>
    </ol>
  </li>
  <li>
    <p><strong>Speaker diarization</strong></p>

    <ol>
      <li>Clustering: Agglomerative hierarchical clustering, spectral clustering,
 Variational Bayes based x-vector clustering (VBx)</li>
      <li>Region proposal networks</li>
      <li>Target speaker voice activity detection</li>
    </ol>
  </li>
  <li>
    <p><strong>Speech recognition (ASR)</strong></p>

    <ol>
      <li>Hybrid TDNNF-based</li>
      <li>End-to-end transformer based</li>
    </ol>
  </li>
</ol>

<p><a name="data"></a></p>

<h3 id="datasets">Datasets</h3>

<p>In our paper, we compare the performance of our models on mixed and separated audio,
where the separation is performed using the methods mentioned earlier.</p>

<h4 id="downloading-and-preparing-the-mixed-original-libricss-data">Downloading and preparing the mixed (original) LibriCSS data</h4>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>git clone https://github.com/chenzhuo1011/libri_css.git
<span class="gp">$ </span>conda env create -f conda_env.yml
<span class="gp">$ </span>conda activate libricss_release
<span class="gp">$ </span><span class="nb">cd </span>libri_css <span class="o">&amp;&amp;</span> ./dataprep/scripts/dataprep.sh
</code></pre>
</div>

<h4 id="downloading-the-separated-audio-data">Downloading the separated audio data</h4>

<p>Additionally, we also provided 2-stream and 3-stream separated audio wav files
through <a href="https://doi.org/10.5281/zenodo.4415163">Zenodo</a>. You can download them
using the following commands:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>wget https://zenodo.org/record/4415163/files/libricss_mvdr_2stream.tar.gz
<span class="gp">$ </span>wget https://zenodo.org/record/4415163/files/libricss_sequential_3stream.tar.gz
</code></pre>
</div>

<p>Once the archived files are downloaded, you can extract them and then use the path
in the Kaldi or ESPNet recipes.</p>

<p><a name="recipe"></a></p>

<h3 id="reproducible-recipes">Reproducible recipes</h3>

<h4 id="kaldi-recipe">Kaldi recipe</h4>

<p>We have provided Kaldi recipes <code class="highlighter-rouge">s5_mono</code> and <code class="highlighter-rouge">s5_css</code> <a href="https://github.com/kaldi-asr/kaldi/tree/master/egs/libri_css">here</a>.</p>

<ol>
  <li>
    <p><code class="highlighter-rouge">s5_mono</code>: This is a single channel diarization + ASR recipe which takes as the
input a long single-channel recording containing mixed audio. It then performs SAD,
diarization, and ASR on it and outputs speaker-attributed transcriptions, 
which are then evaluated with cpWER (similar to CHiME6 Track 2).</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">s5_css</code>: This pipeline uses a speech separation module at the beginning,
so the input is 2-3 separated audio streams. We assume that the separation is
window-based, so that the same speaker may be split across different streams in
different windows, thus making diarization necessary.</p>
  </li>
</ol>

<p><code class="highlighter-rouge">s5_mono</code> evaluates diarization and ASR on mixed audio, while <code class="highlighter-rouge">s5_css</code> does the
same for separated audio streams.</p>

<p><strong>Note:</strong> Only clustering-based diarization is available in this recipe at the 
time of making this post, but we are also preparing RPN and TS-VAD setups.</p>

<p>For ease of reproduction, we have included training stages in the <code class="highlighter-rouge">s5_mono</code> recipe. 
We also provide pretrained models for both diarization and ASR systems:</p>

<ul>
  <li>
    <p><strong>SAD</strong>: CHiME-6 baseline TDNN-Stats SAD available <a href="http://kaldi-asr.org/models/m12">here</a>.</p>
  </li>
  <li>
    <p><strong>Speaker diarization</strong>: CHiME-6 baseline x-vector + AHC diarizer, trained on VoxCeleb 
with simulated RIRs available <a href="http://kaldi-asr.org/models/m12">here</a>.</p>
  </li>
  <li>
    <p><strong>ASR</strong>: We used the chain model trained on 960h clean LibriSpeech training data available
<a href="http://kaldi-asr.org/models/m13">here</a>. It was then additionally fine-tuned for 1
epoch on LibriSpeech + simulated RIRs. For LM, we trained a TDNN-LSTM language model
for rescoring. All of these models are available at this 
<a href="https://drive.google.com/file/d/13ceXdK6oAUuUyxn7kjQVVqpe8r6Sc7ds/view?usp=sharing">Google Drive link</a>.</p>
  </li>
</ul>

<h4 id="espnet-recipe">ESPNet recipe</h4>

<p>The ESPNet recipe corresponding to the Kaldi <code class="highlighter-rouge">s5_mono</code> recipe is available 
<a href="https://github.com/espnet/espnet/tree/master/egs/libri_css">here</a> as <code class="highlighter-rouge">asr1</code>.
A recipe corresponding to <code class="highlighter-rouge">s5_css</code> is not available yet, but it should be simple
to extend <code class="highlighter-rouge">asr1</code> similar to the <code class="highlighter-rouge">s5_css</code> recipe, since it follows Kaldi-style
diarization. For help or other details, please contact 
<a href="https://www.ims.uni-stuttgart.de/en/institute/team/Denisov/">Pavel Denisov</a> 
who created the ESPNet LibriCSS recipe.</p>

<p><a name="example"></a></p>

<h3 id="example-use-cases-for-extending-this-work">Example use cases (for extending this work)</h3>

<p>Let us now look at how this research may be extended through 2 examples.</p>

<h4 id="example-1-a-new-window-based-separation-method">Example 1: A new window-based separation method</h4>

<p>Suppose you have a new <em>window-based</em> “continuous” speech separation model, and 
you want to evaluate the downstream ASR performance (and compare it with the
methods in our paper). This can be done as follows:</p>

<ol>
  <li>
    <p>Download and prepare the “mixed” LibriCSS audio data as described <a href="#data">here</a>.</p>
  </li>
  <li>
    <p>Run your separation method on this data and store the generated audio streams
using the following naming convention: 
<code class="highlighter-rouge">overlap_ratio_10.0_sil0.1_1.0_session7_actual10.1_channel_1.wav</code>. It is similar 
to the naming of the original LibriCSS files, with the addition of <em>_channel_1</em> 
at the end which denotes the stream.</p>

    <blockquote>
      <p><strong>Note</strong>: channel here does not refer to the microphone; it refers to the 
 separated stream; so if your model separated the audio into 2 streams, they 
 would have the suffixes <em>_channel_0</em> and <em>_channel_1</em>.</p>
    </blockquote>
  </li>
  <li>
    <p>Store all the output wav files in a directory. They can have any hierarchy within
this directory as long as they follow the naming convention.</p>
  </li>
  <li>
    <p>Download and install Kaldi, and navigate to the <code class="highlighter-rouge">egs/libri_css/s5_css</code> folder.</p>
  </li>
  <li>
    <p>In the <code class="highlighter-rouge">run.sh</code>, replace the paths to the mixed LibriCSS data and your own
separated audio files.</p>
  </li>
  <li>
    <p>Run the script. It is recommended to run the stages one by one, since the
evaluation outputs after the SAD and diarization stage are also printed to
the standard output.</p>
  </li>
  <li>
    <p>At the end of decoding, the cpWERs will be printed as follows:</p>
  </li>
</ol>

<div class="highlighter-rouge"><pre class="highlight"><code>Dev WERs:
best_wer_session0_CH0_0L %WER 10.98 [ 130 / 1184, 34 ins, 12 del, 84 sub ]
best_wer_session0_CH0_0S %WER 15.10 [ 269 / 1782, 67 ins, 23 del, 179 sub ]
best_wer_session0_CH0_OV10 %WER 25.12 [ 465 / 1851, 156 ins, 85 del, 224 sub ]
best_wer_session0_CH0_OV20 %WER 18.86 [ 342 / 1813, 94 ins, 33 del, 215 sub ]
best_wer_session0_CH0_OV30 %WER 20.42 [ 395 / 1934, 117 ins, 40 del, 238 sub ]
best_wer_session0_CH0_OV40 %WER 28.47 [ 636 / 2234, 236 ins, 137 del, 263 sub ]
Eval WERs:
0L %WER 22.36 [ 2446 / 10938, 785 ins, 413 del, 1248 sub ]
0S %WER 19.81 [ 2970 / 14994, 861 ins, 431 del, 1678 sub ]
OV10 %WER 21.39 [ 3412 / 15951, 1060 ins, 580 del, 1772 sub ]
OV20 %WER 23.49 [ 3984 / 16963, 1128 ins, 747 del, 2109 sub ]
OV30 %WER 26.06 [ 4789 / 18376, 1415 ins, 988 del, 2386 sub ]
OV40 %WER 25.45 [ 4818 / 18932, 1410 ins, 676 del, 2732 sub ]
</code></pre>
</div>

<p>These can also be found at: <code class="highlighter-rouge">exp/chain_cleaned/tdnn_1d_sp/decode_dev${data_affix}_diarized_2stage_rescore/scoring_kaldi_multispeaker/best_wer</code></p>

<p><strong>Note:</strong> To evaluate performance using end-to-end Transformer based ASR, you would need
to first create an <code class="highlighter-rouge">s5_css</code> equivalent in ESPNet by extending the <code class="highlighter-rouge">asr1</code> recipe.</p>

<h4 id="example-2-a-new-cross-stream-diarizer">Example 2: A new cross-stream diarizer</h4>

<p>One of the observations in our paper was that it is hard to perform good diarization
on top of separated audio streams:</p>

<ol>
  <li>
    <p>Methods such as VBx cannot be used for this purpose because of their
time continuity constraint.</p>
  </li>
  <li>
    <p>Although models like RPN and TS-VAD do well on mixed audio, they fail on
separated audio due to a train-test mismatch (they were trained on simulated
overlapping mixtures).</p>
  </li>
</ol>

<p>Yet, this “separation+diarization” system is very promising, especially
considering that such a <a href="https://arxiv.org/pdf/2010.11458v2.pdf">system from Microsoft</a>
obtained the best performance in the recent VoxConverse diarization challenge.</p>

<p>Suppose you have a new diarization method which works across separated audio
streams. To evaluate your method on LibriCSS:</p>

<ol>
  <li>
    <p>Download the separated audio data from Zenodo.</p>
  </li>
  <li>
    <p>If your method is implemented in Kaldi, clone and install Kaldi and follow the
<code class="highlighter-rouge">s5_css</code> recipe until the diarization stage. Otherwise, run your implementation
on the separated audio files and compute the final DER.</p>
  </li>
  <li>
    <p>You can compare your performance against the results reported in Table 3 of
our paper. The baselines can be reproduced by running the <code class="highlighter-rouge">s5_css</code> recipe till
stage 3.</p>
  </li>
</ol>

<p><a name="credits"></a></p>

<h3 id="credits">Credits</h3>

<p>This work was conducted during JSALT 2020 with support from Microsoft, Amazon,
and Google. If you use the data or code in your research, consider citing:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>@article{Raj2021IntegrationOS,
  title={Integration of speech separation, diarization, and recognition for multi-speaker 
  meetings: System description, comparison, and analysis},
  author={Desh Raj and Pavel Denisov and Zhuo Chen and Hakan Erdogan and Zili Huang 
  and Maokui He and Shinji Watanabe and Jun Du and Takuya Yoshioka and Yi Luo and 
  Naoyuki Kanda and Jinyu Li and Scott Wisdom and John R. Hershey},
  journal={2021 IEEE Spoken Language Technology ({SLT}) Workshop},
  year={2021}}
}
</code></pre>
</div>

<h3 id="other-resources">Other resources</h3>

<ul>
  <li>Paper: <a href="https://arxiv.org/abs/2011.02014">Link</a></li>
  <li>Slides: <a href="https://desh2608.github.io/static/ppt/slt21_jsalt_slides.pdf">Link</a></li>
</ul>

	    
    </div>
  </div>
</div>


    </main>

    <footer>
  <div class="container beautiful-jekyll-footer">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          <li>
            <a href="https://github.com/antonioalvarezl" title="GitHub">
              <span class="fa-stack fa-lg" aria-hidden="true">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
              </span>
              <span class="sr-only">GitHub</span>
            </a>
          </li>
          
		  
	  
      
		  
          <li>
            <a href="mailto:antonio.alvarezl@uam.es" title="Email me">
              <span class="fa-stack fa-lg" aria-hidden="true">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
              <span class="sr-only">Email me</span>
            </a>
          </li>
          
		  
          <li>
            <a href="https://linkedin.com/in/antonio-alvarez-lopez-6a09851b4" title="LinkedIn">
              <span class="fa-stack fa-lg" aria-hidden="true">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
              </span>
              <span class="sr-only">LinkedIn</span>
            </a>
          </li>
          
		  
          <li>
            <a href="https://orcid.org/0009-0004-1302-6389" title="ORCID">
              <span class="fa-stack fa-lg" aria-hidden="true">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="ai ai-orcid ai-stack-1x fa-inverse"></i>
              </span>
              <span class="sr-only">ORCID</span>
            </a>
          </li>
          
          
          <li>
            <a href="https://scholar.google.com/citations?user=4Xm2yLgAAAAJ" title="Google Scholar">
              <span class="fa-stack fa-lg" aria-hidden="true">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="ai ai-google-scholar ai-stack-1x fa-inverse"></i>
              </span>
              <span class="sr-only">Google Scholar</span>
            </a>
          </li>
          
          
          <li>
            <a href="https://www.researchgate.net/profile/Antonio-Alvarez-Lopez-2" title="ResearchGate">
              <span class="fa-stack fa-lg" aria-hidden="true">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="ai ai-researchgate ai-stack-1x fa-inverse"></i>
              </span>
              <span class="sr-only">ResearchGate</span>
            </a>
          </li>
          
		  
		  
      
      
      
      
      
		  
        </ul>
        <p class="copyright text-muted">
		  Antonio Álvarez-López
		  &nbsp;&bull;&nbsp;
		  2026

		  
		  &nbsp;&bull;&nbsp;
		  <a href="http://localhost:4000">antonioalvarezl.github.io</a>
		  
	    </p>
	        <!-- Please don't remove this, keep my open source work credited :) -->
		<p class="theme-by text-muted">
		  Theme by
		  <a href="http://deanattali.com/beautiful-jekyll/">beautiful-jekyll</a>
		</p>
      </div>
    </div>
  </div>
</footer>

  
    






  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script>
      	if (typeof jQuery == 'undefined') {
      	  document.write('<script src="/js/jquery-1.11.2.min.js"></scr' + 'ipt>');
      	}
      </script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/bootstrap.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/main.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/pjax.js"></script>
    
  



	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'G-M3M6TNF7N5', 'auto');
		ga('send', 'pageview');
	</script>
	<!-- End Google Analytics -->


</html>
